{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e692b8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\imani\\appdata\\roaming\\python\\python314\\site-packages (2.10.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\imani\\appdata\\roaming\\python\\python314\\site-packages (0.25.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\imani\\appdata\\roaming\\python\\python314\\site-packages (from torch) (3.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\imani\\appdata\\roaming\\python\\python314\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\imani\\appdata\\roaming\\python\\python314\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\imani\\appdata\\roaming\\python\\python314\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\imani\\appdata\\roaming\\python\\python314\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\imani\\appdata\\roaming\\python\\python314\\site-packages (from torch) (2026.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\imani\\appdata\\roaming\\python\\python314\\site-packages (from torch) (82.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\imani\\appdata\\roaming\\python\\python314\\site-packages (from torchvision) (2.4.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\imani\\appdata\\roaming\\python\\python314\\site-packages (from torchvision) (12.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\imani\\appdata\\roaming\\python\\python314\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\imani\\appdata\\roaming\\python\\python314\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: C:\\Python314\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc2ca2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d59f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ComradeRLAgent:\n",
    "    def __init__(self, state_dim=10, action_dim=5):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Actor network (policy): State -> Action\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.actor_target = Actor(state_dim, action_dim)\n",
    "        \n",
    "        # Twin Critic networks (value): State, Action -> Q-value\n",
    "        self.critic_1 = Critic(state_dim, action_dim)\n",
    "        self.critic_2 = Critic(state_dim, action_dim)\n",
    "        self.critic_1_target = Critic(state_dim, action_dim)\n",
    "        self.critic_2_target = Critic(state_dim, action_dim)\n",
    "        \n",
    "        # Hierarchical tier policy for discrete decisions\n",
    "        self.tier_policy = TierPolicyNetwork(state_dim)\n",
    "        \n",
    "        # Replay buffer with prioritized experience replay\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(capacity=1000000)\n",
    "        \n",
    "        # Hyperparameters tuned for pricing dynamics\n",
    "        self.gamma = 0.99          # Discount factor\n",
    "        self.tau = 0.005           # Soft update rate\n",
    "        self.policy_noise = 0.2    # Target policy smoothing\n",
    "        self.noise_clip = 0.5      # Noise clipping\n",
    "        self.policy_freq = 2       # Delayed policy updates\n",
    "        self.batch_size = 256\n",
    "        self.exploration_noise = 0.3\n",
    "        \n",
    "        self.total_it = 0\n",
    "        \n",
    "    def select_action(self, state, evaluate=False):\n",
    "        \"\"\"\n",
    "        Hierarchical action selection:\n",
    "        1. Tier decision (discrete) - which tier to optimize for\n",
    "        2. Continuous action (price adjustment, notification, etc.)\n",
    "        \"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        # Tier selection (discrete action via Gumbel-Softmax)\n",
    "        tier_logits = self.tier_policy(state_tensor)\n",
    "        if evaluate:\n",
    "            tier = torch.argmax(tier_logits, dim=-1)\n",
    "        else:\n",
    "            tier = F.gumbel_softmax(tier_logits, hard=True)\n",
    "        \n",
    "        # Continuous action from actor\n",
    "        action = self.actor(state_tensor, tier).cpu().data.numpy().flatten()\n",
    "        \n",
    "        if not evaluate:\n",
    "            # Add exploration noise (Ornstein-Uhlenbeck for temporal correlation)\n",
    "            noise = self.ou_noise.sample()\n",
    "            action = action + noise * self.exploration_noise\n",
    "            action = np.clip(action, -1, 1)  # Action space normalized\n",
    "        \n",
    "        # Denormalize actions\n",
    "        action_denorm = self.denormalize_action(action, tier)\n",
    "        \n",
    "        return action_denorm, tier\n",
    "    \n",
    "    def denormalize_action(self, action, tier):\n",
    "        \"\"\"Convert normalized [-1, 1] actions to actual values\"\"\"\n",
    "        tier_config = TIER_CONFIGS[tier]\n",
    "        \n",
    "        return {\n",
    "            'delta_price': np.interp(action[0], [-1, 1], [-20, 20]),  # ±$20\n",
    "            'delta_group_target': int(np.interp(action[1], [-1, 1], [-2, 2])),  # ±2 members\n",
    "            'notification_push': int(np.interp(action[2], [-1, 1], [0, tier_config['max_notifications']])),\n",
    "            'promo_duration': int(np.interp(action[3], [-1, 1], [0, 72])),  # 0-72 hours\n",
    "            'match_aggressiveness': np.interp(action[4], [-1, 1], [0, 1])  # 0-1 matching score\n",
    "        }\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"TD3 training with twin critics and delayed policy updates\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return {}\n",
    "        \n",
    "        self.total_it += 1\n",
    "        \n",
    "        # Sample with priorities\n",
    "        batch, indices, weights = self.replay_buffer.sample(self.batch_size)\n",
    "        state, action, reward, next_state, done = batch\n",
    "        \n",
    "        # Add target policy smoothing\n",
    "        noise = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "        next_action = (self.actor_target(next_state) + noise).clamp(-1, 1)\n",
    "        \n",
    "        # Compute target Q-value (minimum of twin critics)\n",
    "        target_q1 = self.critic_1_target(next_state, next_action)\n",
    "        target_q2 = self.critic_2_target(next_state, next_action)\n",
    "        target_q = torch.min(target_q1, target_q2)\n",
    "        target_q = reward + (1 - done) * self.gamma * target_q\n",
    "        \n",
    "        # Update critics\n",
    "        current_q1 = self.critic_1(state, action)\n",
    "        current_q2 = self.critic_2(state, action)\n",
    "        \n",
    "        critic_loss = F.mse_loss(current_q1, target_q, reduction='none') * weights\n",
    "        critic_loss = critic_loss.mean() + F.mse_loss(current_q2, target_q, reduction='none') * weights.mean()\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_1.parameters(), max_norm=1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_2.parameters(), max_norm=1.0)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Delayed policy updates\n",
    "        actor_loss = None\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "            # Actor loss: maximize Q-value\n",
    "            actor_action = self.actor(state)\n",
    "            actor_loss = -self.critic_1(state, actor_action).mean()\n",
    "            \n",
    "            # Add entropy bonus for exploration\n",
    "            entropy = -torch.sum(actor_action * torch.log(actor_action + 1e-10), dim=-1).mean()\n",
    "            actor_loss += 0.01 * entropy\n",
    "            \n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            \n",
    "            # Soft update target networks\n",
    "            self.soft_update()\n",
    "        \n",
    "        # Update priorities in replay buffer\n",
    "        priorities = torch.abs(current_q1 - target_q).detach().cpu().numpy() + 1e-6\n",
    "        self.replay_buffer.update_priorities(indices, priorities)\n",
    "        \n",
    "        return {\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'actor_loss': actor_loss.item() if actor_loss else None,\n",
    "            'q_value': current_q1.mean().item()\n",
    "        }\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Policy network with tier-conditioned output\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim + 4, hidden_dim),  # +4 for tier one-hot\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Price adjustment head (careful, market-sensitive)\n",
    "        self.price_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Tanh()  # [-1, 1] normalized\n",
    "        )\n",
    "        \n",
    "        # Group target head (discrete steps)\n",
    "        self.group_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Notification head (resource allocation)\n",
    "        self.notify_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()  # [0, 1] -> scale to max\n",
    "        )\n",
    "        \n",
    "        # Promotion head (time-limited)\n",
    "        self.promo_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Matching head (supplier-user fit)\n",
    "        self.match_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, tier):\n",
    "        tier_onehot = F.one_hot(tier, num_classes=4).float()\n",
    "        x = torch.cat([state, tier_onehot], dim=-1)\n",
    "        x = self.shared(x)\n",
    "        \n",
    "        return torch.cat([\n",
    "            self.price_head(x),\n",
    "            self.group_head(x),\n",
    "            self.notify_head(x),\n",
    "            self.promo_head(x),\n",
    "            self.match_head(x)\n",
    "        ], dim=-1)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Twin Q-networks for reduced overestimation bias\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Q1 architecture\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.q1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62db0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurriculumTrainer:\n",
    "    \"\"\"\n",
    "    Train RL agent through progressively complex scenarios:\n",
    "    1. Single tier, static demand (convergence)\n",
    "    2. Single tier, dynamic demand (adaptation)\n",
    "    3. Multi-tier, single supplier (progression)\n",
    "    4. Multi-tier, multi-supplier (competition)\n",
    "    5. Full market with external shocks (robustness)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.curriculum_stages = [\n",
    "            {\n",
    "                'name': 'Static Free Tier',\n",
    "                'duration': 1000,\n",
    "                'config': {\n",
    "                    'demand_volatility': 0.1,\n",
    "                    'supplier_count': 1,\n",
    "                    'tier_options': ['Free'],\n",
    "                    'market_shocks': False,\n",
    "                    'competitor_presence': False\n",
    "                },\n",
    "                'success_threshold': {'avg_reward': 50, 'stability': 0.9}\n",
    "            },\n",
    "            {\n",
    "                'name': 'Dynamic Free Tier',\n",
    "                'duration': 2000,\n",
    "                'config': {\n",
    "                    'demand_volatility': 0.3,\n",
    "                    'supplier_count': 2,\n",
    "                    'tier_options': ['Free'],\n",
    "                    'market_shocks': True,\n",
    "                    'competitor_presence': False\n",
    "                },\n",
    "                'success_threshold': {'avg_reward': 80, 'stability': 0.85}\n",
    "            },\n",
    "            {\n",
    "                'name': 'Tier Progression',\n",
    "                'duration': 3000,\n",
    "                'config': {\n",
    "                    'demand_volatility': 0.4,\n",
    "                    'supplier_count': 3,\n",
    "                    'tier_options': ['Free', 'Standard', 'Premium'],\n",
    "                    'market_shocks': True,\n",
    "                    'competitor_presence': True\n",
    "                },\n",
    "                'success_threshold': {'avg_reward': 120, 'tier_upgrade_rate': 0.3}\n",
    "            },\n",
    "            {\n",
    "                'name': 'Full Market',\n",
    "                'duration': 5000,\n",
    "                'config': {\n",
    "                    'demand_volatility': 0.5,\n",
    "                    'supplier_count': 5,\n",
    "                    'tier_options': ['Free', 'Standard', 'Premium', 'Gold'],\n",
    "                    'market_shocks': True,\n",
    "                    'competitor_presence': True,\n",
    "                    'seasonality': True\n",
    "                },\n",
    "                'success_threshold': {'avg_reward': 200, 'platform_profit': 1000}\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    def train(self, agent):\n",
    "        for stage_idx, stage in enumerate(self.curriculum_stages):\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"STAGE {stage_idx + 1}: {stage['name']}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            env = ComradeEnvironment(stage['config'])\n",
    "            episode_rewards = []\n",
    "            \n",
    "            for episode in range(stage['duration']):\n",
    "                state = env.reset()\n",
    "                episode_reward = 0\n",
    "                done = False\n",
    "                \n",
    "                while not done:\n",
    "                    action = agent.select_action(state, evaluate=False)\n",
    "                    next_state, reward, done, info = env.step(action)\n",
    "                    \n",
    "                    # Store transition with curriculum stage label\n",
    "                    agent.replay_buffer.add((\n",
    "                        state, action, reward, next_state, done,\n",
    "                        {'stage': stage_idx, 'difficulty': stage_idx / 4}\n",
    "                    ))\n",
    "                    \n",
    "                    # Train agent\n",
    "                    metrics = agent.train()\n",
    "                    \n",
    "                    state = next_state\n",
    "                    episode_reward += reward\n",
    "                \n",
    "                episode_rewards.append(episode_reward)\n",
    "                \n",
    "                # Progress logging\n",
    "                if episode % 100 == 0:\n",
    "                    avg_reward = np.mean(episode_rewards[-100:])\n",
    "                    print(f\"Episode {episode}: Avg Reward = {avg_reward:.2f}, \"\n",
    "                          f\"Epsilon = {agent.exploration_noise:.3f}\")\n",
    "                \n",
    "                # Automatic stage progression check\n",
    "                if self.check_stage_completion(episode_rewards, stage['success_threshold']):\n",
    "                    print(f\"✓ Stage {stage['name']} completed at episode {episode}\")\n",
    "                    # Save checkpoint\n",
    "                    agent.save(f\"checkpoint_stage_{stage_idx}.pt\")\n",
    "                    break\n",
    "            \n",
    "            # Decay exploration for next stage\n",
    "            agent.exploration_noise *= 0.8\n",
    "    \n",
    "    def check_stage_completion(self, rewards, thresholds):\n",
    "        \"\"\"Check if agent has mastered current stage\"\"\"\n",
    "        if len(rewards) < 100:\n",
    "            return False\n",
    "        \n",
    "        recent_rewards = rewards[-100:]\n",
    "        avg_reward = np.mean(recent_rewards)\n",
    "        stability = 1 - (np.std(recent_rewards) / (abs(avg_reward) + 1e-6))\n",
    "        \n",
    "        checks = []\n",
    "        if 'avg_reward' in thresholds:\n",
    "            checks.append(avg_reward > thresholds['avg_reward'])\n",
    "        if 'stability' in thresholds:\n",
    "            checks.append(stability > thresholds['stability'])\n",
    "        \n",
    "        return all(checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7987c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionPricingEngine:\n",
    "    \"\"\"\n",
    "    Deployed RL agent with safety guards and A/B testing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        self.agent = ComradeRLAgent()\n",
    "        self.agent.load(model_path)\n",
    "        self.agent.actor.eval()  # Inference mode\n",
    "        \n",
    "        # Safety constraints (hard limits)\n",
    "        self.price_bounds = {\n",
    "            'min_margin': 0.05,  # 5% minimum supplier margin\n",
    "            'max_discount': 0.60,  # 60% max discount from retail\n",
    "            'max_price_change': 0.20  # 20% max change per hour\n",
    "        }\n",
    "        \n",
    "        # A/B testing framework\n",
    "        self.experiment_assignments = {}\n",
    "        \n",
    "        # Fallback rules (if ML fails)\n",
    "        self.fallback_pricing = DifferentialEquationPricing()\n",
    "        \n",
    "    def get_price_recommendation(self, context):\n",
    "        \"\"\"\n",
    "        Real-time price recommendation with safety checks\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Build state vector from real-time data\n",
    "            state = self.build_state_vector(context)\n",
    "            \n",
    "            # Get RL action\n",
    "            action, tier = self.agent.select_action(state, evaluate=True)\n",
    "            \n",
    "            # Apply safety constraints\n",
    "            safe_action = self.apply_safety_constraints(action, context)\n",
    "            \n",
    "            # A/B test: 10% of traffic uses fallback for comparison\n",
    "            if self.should_use_fallback(context['user_id']):\n",
    "                safe_action = self.fallback_pricing.compute(context)\n",
    "                source = 'fallback'\n",
    "            else:\n",
    "                source = 'rl_agent'\n",
    "            \n",
    "            # Log for monitoring\n",
    "            self.log_decision(context, state, action, safe_action, source)\n",
    "            \n",
    "            return {\n",
    "                'price': safe_action['price'],\n",
    "                'target_group_size': safe_action['group_target'],\n",
    "                'notification_strategy': safe_action['notifications'],\n",
    "                'promo_duration': safe_action['promo'],\n",
    "                'confidence': self.compute_confidence(state),\n",
    "                'source': source\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Graceful degradation\n",
    "            return self.fallback_pricing.compute(context)\n",
    "    \n",
    "    def apply_safety_constraints(self, action, context):\n",
    "        \"\"\"Hard constraints to prevent harmful pricing\"\"\"\n",
    "        current_price = context['current_price']\n",
    "        proposed_price = action['price']\n",
    "        \n",
    "        # Constraint 1: Max price change rate\n",
    "        max_change = current_price * self.price_bounds['max_price_change']\n",
    "        if abs(proposed_price - current_price) > max_change:\n",
    "            proposed_price = current_price + np.sign(proposed_price - current_price) * max_change\n",
    "        \n",
    "        # Constraint 2: Minimum supplier margin\n",
    "        cost = context['supplier_cost']\n",
    "        min_price = cost / (1 - self.price_bounds['min_margin'])\n",
    "        proposed_price = max(proposed_price, min_price)\n",
    "        \n",
    "        # Constraint 3: Maximum discount\n",
    "        retail = context['retail_price']\n",
    "        max_discount_price = retail * (1 - self.price_bounds['max_discount'])\n",
    "        proposed_price = max(proposed_price, max_discount_price)\n",
    "        \n",
    "        action['price'] = round(proposed_price, 2)\n",
    "        return action\n",
    "    \n",
    "    def online_learning_update(self, feedback_batch):\n",
    "        \"\"\"\n",
    "        Continual learning from production feedback\n",
    "        \"\"\"\n",
    "        # Experience replay from production\n",
    "        for feedback in feedback_batch:\n",
    "            self.agent.replay_buffer.add(feedback)\n",
    "        \n",
    "        # Conservative update (small learning rate)\n",
    "        for _ in range(10):  # Limited updates per batch\n",
    "            metrics = self.agent.train()\n",
    "        \n",
    "        # Evaluate on holdout set\n",
    "        eval_reward = self.evaluate_on_historical_data()\n",
    "        \n",
    "        # Only deploy if improvement > 2%\n",
    "        if eval_reward > self.current_performance * 1.02:\n",
    "            self.deploy_update()\n",
    "            return True\n",
    "        else:\n",
    "            return False  # Rollback to previous version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
