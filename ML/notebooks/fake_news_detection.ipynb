{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fake News Detection Model\n",
                "\n",
                "This notebook develops a fake news detection model using transfer learning.\n",
                "\n",
                "## Approach\n",
                "1. Load pre-trained DistilBERT model (lightweight transformer)\n",
                "2. Combine Kaggle Fake News dataset with X/Twitter data\n",
                "3. Fine-tune on our combined dataset\n",
                "4. Evaluate and optimize for inference speed\n",
                "\n",
                "## Data Sources\n",
                "- Kaggle Fake News Dataset\n",
                "- X/Twitter API (trending topics)\n",
                "- News API for verified sources"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install transformers datasets torch scikit-learn pandas numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from datasets import load_dataset, Dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer, \n",
                "    AutoModelForSequenceClassification,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                "    DataCollatorWithPadding\n",
                ")\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
                "import torch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load and Prepare Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Kaggle Fake News dataset from HuggingFace\n",
                "# Alternative: Download from Kaggle and load locally\n",
                "\n",
                "try:\n",
                "    # Try loading from HuggingFace datasets\n",
                "    dataset = load_dataset('GonzaloA/fake_news', split='train')\n",
                "    print(f\"Loaded {len(dataset)} samples from HuggingFace\")\n",
                "except:\n",
                "    print(\"Loading from local CSV...\")\n",
                "    # Load from local file if available\n",
                "    # dataset = pd.read_csv('../data/fake_news.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare dataset\n",
                "def prepare_data(examples):\n",
                "    \"\"\"Combine title and text for classification\"\"\"\n",
                "    texts = []\n",
                "    for title, text in zip(examples['title'], examples['text']):\n",
                "        combined = f\"{title}\\n\\n{text[:500]}\"  # Limit text length\n",
                "        texts.append(combined)\n",
                "    return {'text': texts, 'label': examples['label']}\n",
                "\n",
                "# Apply preprocessing\n",
                "# processed_dataset = dataset.map(prepare_data, batched=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Pre-trained Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use DistilBERT for efficiency\n",
                "MODEL_NAME = \"distilbert-base-uncased\"\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    num_labels=2,  # Real (0) or Fake (1)\n",
                "    id2label={0: \"real\", 1: \"fake\"},\n",
                "    label2id={\"real\": 0, \"fake\": 1}\n",
                ")\n",
                "\n",
                "print(f\"Model loaded: {MODEL_NAME}\")\n",
                "print(f\"Parameters: {model.num_parameters():,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tokenization function\n",
                "def tokenize_function(examples):\n",
                "    return tokenizer(\n",
                "        examples['text'],\n",
                "        padding='max_length',\n",
                "        truncation=True,\n",
                "        max_length=256  # Keep short for efficiency\n",
                "    )\n",
                "\n",
                "# tokenized_dataset = processed_dataset.map(tokenize_function, batched=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute metrics\n",
                "def compute_metrics(eval_pred):\n",
                "    predictions, labels = eval_pred\n",
                "    predictions = np.argmax(predictions, axis=1)\n",
                "    \n",
                "    accuracy = accuracy_score(labels, predictions)\n",
                "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
                "        labels, predictions, average='binary'\n",
                "    )\n",
                "    \n",
                "    return {\n",
                "        'accuracy': accuracy,\n",
                "        'precision': precision,\n",
                "        'recall': recall,\n",
                "        'f1': f1\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training arguments optimized for efficiency\n",
                "training_args = TrainingArguments(\n",
                "    output_dir='../models/fake_news',\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=32,\n",
                "    warmup_steps=500,\n",
                "    weight_decay=0.01,\n",
                "    logging_dir='../models/fake_news/logs',\n",
                "    logging_steps=100,\n",
                "    evaluation_strategy='epoch',\n",
                "    save_strategy='epoch',\n",
                "    load_best_model_at_end=True,\n",
                "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize trainer\n",
                "# trainer = Trainer(\n",
                "#     model=model,\n",
                "#     args=training_args,\n",
                "#     train_dataset=tokenized_dataset['train'],\n",
                "#     eval_dataset=tokenized_dataset['test'],\n",
                "#     tokenizer=tokenizer,\n",
                "#     compute_metrics=compute_metrics,\n",
                "#     data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
                "# )\n",
                "\n",
                "# # Train\n",
                "# trainer.train()\n",
                "\n",
                "print(\"Training code ready - uncomment to run with actual data\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save and Export Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model\n",
                "# model.save_pretrained('../models/fake_news/final')\n",
                "# tokenizer.save_pretrained('../models/fake_news/final')\n",
                "\n",
                "print(\"Model export code ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Quick Inference Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_fake_news(text):\n",
                "    \"\"\"Quick inference function\"\"\"\n",
                "    inputs = tokenizer(\n",
                "        text,\n",
                "        return_tensors='pt',\n",
                "        truncation=True,\n",
                "        max_length=256\n",
                "    )\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "        probs = torch.softmax(outputs.logits, dim=1)\n",
                "        prediction = torch.argmax(probs, dim=1).item()\n",
                "        confidence = probs[0][prediction].item()\n",
                "    \n",
                "    return {\n",
                "        'is_fake': prediction == 1,\n",
                "        'confidence': confidence,\n",
                "        'label': 'fake' if prediction == 1 else 'real'\n",
                "    }\n",
                "\n",
                "# Test\n",
                "# result = predict_fake_news(\"Breaking: Scientists discover cure for all diseases!\")\n",
                "# print(result)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}